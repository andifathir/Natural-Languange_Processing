{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e436a41a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    Text  Category  \\\n",
      "0      Are you ok with the DOD removing articles from...  No Slang   \n",
      "1                                                    NaN  No Slang   \n",
      "2                Navajo code talkers are still not back.  No Slang   \n",
      "3      Iâ€™m just thinking about how these are the same...  No Slang   \n",
      "4      Anyone who is okay with this does not respect ...  No Slang   \n",
      "...                                                  ...       ...   \n",
      "97785  This is why the DA will usually bring multiple...  No Slang   \n",
      "97786  True or until the two parties reach a plea agr...  No Slang   \n",
      "97787  Not all public defenders are state employees. ...  No Slang   \n",
      "97788  To hear my buddy tell it, that wasn't the issu...  No Slang   \n",
      "97789  Hmmm, usually ignorance of the law is not a de...  No Slang   \n",
      "\n",
      "                      Date          Subreddit  \n",
      "0      2025-03-18 12:05:01          AskReddit  \n",
      "1      2025-03-18 12:05:01          AskReddit  \n",
      "2      2025-03-18 12:33:08          AskReddit  \n",
      "3      2025-03-18 12:21:21          AskReddit  \n",
      "4      2025-03-18 12:29:26          AskReddit  \n",
      "...                    ...                ...  \n",
      "97785  2025-03-18 12:28:48  NoStupidQuestions  \n",
      "97786  2025-03-18 13:28:46  NoStupidQuestions  \n",
      "97787  2025-03-18 15:32:23  NoStupidQuestions  \n",
      "97788  2025-03-18 12:33:08  NoStupidQuestions  \n",
      "97789  2025-03-18 16:06:33  NoStupidQuestions  \n",
      "\n",
      "[97790 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "df = pd.read_csv('reduced_output_file.csv')  # Replace 'your_file.csv' with your actual file path\n",
    "\n",
    "# Create DataFrame\n",
    "post_data = pd.DataFrame(df)\n",
    "\n",
    "# Step 1: Combine Title, Text, and Comments Data into separate rows\n",
    "\n",
    "# Convert Comments Data from string format to list\n",
    "post_data['Comments Data'] = post_data['Comments Data'].apply(ast.literal_eval)\n",
    "\n",
    "# Prepare the DataFrame for expanding to multiple rows\n",
    "expanded_data = []\n",
    "\n",
    "# Add the Title and Text as separate rows\n",
    "for _, row in post_data.iterrows():\n",
    "    # Add the title as a row\n",
    "    expanded_data.append({'Text': row['Title'], 'Category': 'No Slang', 'Date': datetime.fromtimestamp(row['Timestamp'], tz=timezone.utc).strftime('%Y-%m-%d %H:%M:%S'), 'Subreddit': row['Subreddit']})\n",
    "    \n",
    "    # Add the text (if available)\n",
    "    if row['Text']:\n",
    "        expanded_data.append({'Text': row['Text'], 'Category': 'No Slang', 'Date': datetime.fromtimestamp(row['Timestamp'], tz=timezone.utc).strftime('%Y-%m-%d %H:%M:%S'), 'Subreddit': row['Subreddit']})\n",
    "    \n",
    "    # Add each comment as a separate row\n",
    "    for comment in row['Comments Data']:\n",
    "        expanded_data.append({'Text': comment[0], 'Category': 'No Slang', 'Date': datetime.fromtimestamp(comment[2], tz=timezone.utc).strftime('%Y-%m-%d %H:%M:%S'), 'Subreddit': row['Subreddit']})\n",
    "\n",
    "# Create the expanded DataFrame\n",
    "expanded_df = pd.DataFrame(expanded_data)\n",
    "\n",
    "expanded_df.to_csv('cleaned_data.csv', index=False)  # Saves the cleaned data to a new file\n",
    "\n",
    "print(expanded_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "81b1e1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Remove duplicates\n",
    "expanded_df = expanded_df.drop_duplicates()\n",
    "\n",
    "# Handle missing values in 'Text' or 'Category' column by removing rows with missing data\n",
    "expanded_df = expanded_df.dropna(subset=['Text', 'Category'])\n",
    "\n",
    "# Clean text (removing special characters, URLs, etc.)\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)  # Remove URLs\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)  # Remove non-alphanumeric characters\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Replace multiple spaces with a single space\n",
    "    return text.lower().strip()\n",
    "\n",
    "expanded_df['Text'] = expanded_df['Text'].apply(clean_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c365bbbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\andif\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\andif\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\andif\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Tokenization and stopword removal\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stopwords and non-alphabetical words\n",
    "    tokens = [word.lower() for word in tokens if word.lower() not in stop_words and word.isalpha()]\n",
    "    # Lemmatization\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    return \" \".join(lemmatized_tokens)\n",
    "\n",
    "expanded_df['Text'] = expanded_df['Text'].apply(preprocess_text)\n",
    "\n",
    "# Text Vectorization (TF-IDF)\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X = tfidf_vectorizer.fit_transform(expanded_df['Text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cc9eec5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# Apply Truncated SVD for dimensionality reduction\n",
    "svd = TruncatedSVD(n_components=100)  # Adjust n_components as needed\n",
    "X_reduced = svd.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c411cd9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discretize_length(text):\n",
    "    length = len(text.split())\n",
    "    if length < 20:\n",
    "        return 'Short'\n",
    "    elif length < 50:\n",
    "        return 'Medium'\n",
    "    else:\n",
    "        return 'Long'\n",
    "\n",
    "expanded_df['Text Length'] = expanded_df['Text'].apply(discretize_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4261517",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['Upvotes'], dtype='object')] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Assuming 'Upvotes' is a numerical feature\u001b[39;00m\n\u001b[32m      4\u001b[39m scaler = StandardScaler()\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m expanded_df[\u001b[33m'\u001b[39m\u001b[33mUpvotes Normalized\u001b[39m\u001b[33m'\u001b[39m] = scaler.fit_transform(\u001b[43mexpanded_df\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mUpvotes\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\personal\\Code\\S6\\NLP\\.venv\\Lib\\site-packages\\pandas\\core\\frame.py:4108\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4106\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[32m   4107\u001b[39m         key = \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[32m-> \u001b[39m\u001b[32m4108\u001b[39m     indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcolumns\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[32m1\u001b[39m]\n\u001b[32m   4110\u001b[39m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[32m   4111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[33m\"\u001b[39m\u001b[33mdtype\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) == \u001b[38;5;28mbool\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\personal\\Code\\S6\\NLP\\.venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6200\u001b[39m, in \u001b[36mIndex._get_indexer_strict\u001b[39m\u001b[34m(self, key, axis_name)\u001b[39m\n\u001b[32m   6197\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   6198\u001b[39m     keyarr, indexer, new_indexer = \u001b[38;5;28mself\u001b[39m._reindex_non_unique(keyarr)\n\u001b[32m-> \u001b[39m\u001b[32m6200\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   6202\u001b[39m keyarr = \u001b[38;5;28mself\u001b[39m.take(indexer)\n\u001b[32m   6203\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[32m   6204\u001b[39m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\personal\\Code\\S6\\NLP\\.venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6249\u001b[39m, in \u001b[36mIndex._raise_if_missing\u001b[39m\u001b[34m(self, key, indexer, axis_name)\u001b[39m\n\u001b[32m   6247\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m nmissing:\n\u001b[32m   6248\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m nmissing == \u001b[38;5;28mlen\u001b[39m(indexer):\n\u001b[32m-> \u001b[39m\u001b[32m6249\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m]\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   6251\u001b[39m     not_found = \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask.nonzero()[\u001b[32m0\u001b[39m]].unique())\n\u001b[32m   6252\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m not in index\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyError\u001b[39m: \"None of [Index(['Upvotes'], dtype='object')] are in the [columns]\""
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
