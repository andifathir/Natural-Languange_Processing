{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "309d778f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6920 entries, 0 to 6919\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   Text       6920 non-null   object\n",
      " 1   Subreddit  6920 non-null   object\n",
      " 2   Category   6920 non-null   object\n",
      "dtypes: object(3)\n",
      "memory usage: 162.3+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None,\n",
       "                                                 Text  Subreddit  \\\n",
       " 0  this man is a cut above the rest outdoor or in...     soccer   \n",
       " 1                                       why not four      funny   \n",
       " 2                                          maga baby  AskReddit   \n",
       " 3  i doubt even chatgpt would come up with a stor...     gaming   \n",
       " 4  i feel thats kinda true but at the same time a...     soccer   \n",
       " \n",
       "           Category  \n",
       " 0         No Slang  \n",
       " 1         No Slang  \n",
       " 2  Offensive Slang  \n",
       " 3         No Slang  \n",
       " 4         No Slang  )"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "file_path = \"C:/personal/Code/S6/NLP/chatgpt_prelabel.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Display basic info and the first few rows\n",
    "df_info = df.info()\n",
    "df_head = df.head()\n",
    "\n",
    "df_info, df_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d798c71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Category\"] = df[\"Category\"].str.replace(r\"\\([^)]*\\)\", \"\", regex=True).str.strip()\n",
    "df.to_csv('chatgpt_prelabel.csv', index=False)  # Saves the cleaned data to a new file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9fe81d3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data reduced and saved to reduced_output.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def reduce_csv_to_1000_rows(input_csv, output_csv):\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(input_csv)\n",
    "\n",
    "    # If the dataset has more than 1000 rows, randomly sample 1000 rows\n",
    "    if len(df) > 1000:\n",
    "        df_reduced = df.sample(n=7100, random_state=42)  # random_state for reproducibility\n",
    "    else:\n",
    "        df_reduced = df  # if there are fewer than 1000 rows, keep all rows\n",
    "\n",
    "    # Save the reduced dataset to a new CSV file\n",
    "    df_reduced.to_csv(output_csv, index=False)\n",
    "\n",
    "    print(f\"Data reduced and saved to {output_csv}\")\n",
    "\n",
    "# Example usage\n",
    "input_csv = 'merge_all_text.csv'  # Replace with your input CSV file path\n",
    "output_csv = 'reduced_output.csv'  # Replace with your desired output file path\n",
    "reduce_csv_to_1000_rows(input_csv, output_csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "47744748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    Text  Category  \\\n",
      "0      Are you ok with the DOD removing articles from...  No Slang   \n",
      "1                                                    NaN  No Slang   \n",
      "2                Navajo code talkers are still not back.  No Slang   \n",
      "3      Iâ€™m just thinking about how these are the same...  No Slang   \n",
      "4      Anyone who is okay with this does not respect ...  No Slang   \n",
      "...                                                  ...       ...   \n",
      "97785  This is why the DA will usually bring multiple...  No Slang   \n",
      "97786  True or until the two parties reach a plea agr...  No Slang   \n",
      "97787  Not all public defenders are state employees. ...  No Slang   \n",
      "97788  To hear my buddy tell it, that wasn't the issu...  No Slang   \n",
      "97789  Hmmm, usually ignorance of the law is not a de...  No Slang   \n",
      "\n",
      "                      Date  Upvotes          Subreddit  \n",
      "0      2025-03-18 12:05:01     7614          AskReddit  \n",
      "1      2025-03-18 12:05:01     7614          AskReddit  \n",
      "2      2025-03-18 12:33:08     2571          AskReddit  \n",
      "3      2025-03-18 12:21:21     5572          AskReddit  \n",
      "4      2025-03-18 12:29:26     1696          AskReddit  \n",
      "...                    ...      ...                ...  \n",
      "97785  2025-03-18 12:28:48        0  NoStupidQuestions  \n",
      "97786  2025-03-18 13:28:46        1  NoStupidQuestions  \n",
      "97787  2025-03-18 15:32:23        3  NoStupidQuestions  \n",
      "97788  2025-03-18 12:33:08        5  NoStupidQuestions  \n",
      "97789  2025-03-18 16:06:33        1  NoStupidQuestions  \n",
      "\n",
      "[97790 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "df = pd.read_csv('Original Data.csv')  # Replace 'your_file.csv' with your actual file path\n",
    "\n",
    "# Create DataFrame\n",
    "post_data = pd.DataFrame(df)\n",
    "\n",
    "# Step 1: Combine Title, Text, and Comments Data into separate rows\n",
    "\n",
    "# Convert Comments Data from string format to list\n",
    "post_data['Comments Data'] = post_data['Comments Data'].apply(ast.literal_eval)\n",
    "\n",
    "# Prepare the DataFrame for expanding to multiple rows\n",
    "expanded_data = []\n",
    "\n",
    "# Add the Title and Text as separate rows\n",
    "for _, row in post_data.iterrows():\n",
    "    # Add the title as a row\n",
    "    expanded_data.append({'Text': row['Title'], 'Category': 'No Slang', 'Date': datetime.fromtimestamp(row['Timestamp'], tz=timezone.utc).strftime('%Y-%m-%d %H:%M:%S'), 'Upvotes': row['Upvotes'], 'Subreddit': row['Subreddit']})\n",
    "    \n",
    "    # Add the text (if available)\n",
    "    if row['Text']:\n",
    "        expanded_data.append({'Text': row['Text'], 'Category': 'No Slang', 'Date': datetime.fromtimestamp(row['Timestamp'], tz=timezone.utc).strftime('%Y-%m-%d %H:%M:%S'), 'Upvotes': row['Upvotes'], 'Subreddit': row['Subreddit']})\n",
    "    \n",
    "    # Add each comment as a separate row\n",
    "    for comment in row['Comments Data']:\n",
    "        expanded_data.append({'Text': comment[0], 'Category': 'No Slang', 'Date': datetime.fromtimestamp(comment[2], tz=timezone.utc).strftime('%Y-%m-%d %H:%M:%S'), 'Upvotes': comment[1], 'Subreddit': row['Subreddit']})\n",
    "\n",
    "# Create the expanded DataFrame\n",
    "expanded_df = pd.DataFrame(expanded_data)\n",
    "\n",
    "expanded_df.to_csv('merge_all_text.csv', index=False)  # Saves the cleaned data to a new file\n",
    "\n",
    "print(expanded_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b15885f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data reduced and saved to 10_sample.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def reduce_csv_to_1000_rows(input_csv, output_csv):\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(input_csv)\n",
    "\n",
    "    # If the dataset has more than 1000 rows, randomly sample 1000 rows\n",
    "    if len(df) > 1000:\n",
    "        df_reduced = df.sample(n=10, random_state=42)  # random_state for reproducibility\n",
    "    else:\n",
    "        df_reduced = df  # if there are fewer than 1000 rows, keep all rows\n",
    "\n",
    "    # Save the reduced dataset to a new CSV file\n",
    "    df_reduced.to_csv(output_csv, index=False)\n",
    "\n",
    "    print(f\"Data reduced and saved to {output_csv}\")\n",
    "\n",
    "# Example usage\n",
    "input_csv = 'target_data.csv'  # Replace with your input CSV file path\n",
    "output_csv = '10_sample.csv'  # Replace with your desired output file path\n",
    "reduce_csv_to_1000_rows(input_csv, output_csv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0ed6d3",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "dfee42c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7100 entries, 0 to 7099\n",
      "Data columns (total 5 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   Text       7057 non-null   object\n",
      " 1   Category   7100 non-null   object\n",
      " 2   Date       7100 non-null   object\n",
      " 3   Upvotes    7100 non-null   int64 \n",
      " 4   Subreddit  7100 non-null   object\n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 277.5+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None,\n",
       "                                                 Text  Category  \\\n",
       " 0  This man is a cut above the rest, outdoor or i...  No Slang   \n",
       " 1                                      Why not four?  No Slang   \n",
       " 2                                          MAGA baby  No Slang   \n",
       " 3  I doubt even ChatGPT would come up with a stor...  No Slang   \n",
       " 4  I feel that's kinda true but at the same time ...  No Slang   \n",
       " \n",
       "                   Date  Upvotes  Subreddit  \n",
       " 0  2025-03-18 12:36:49        1     soccer  \n",
       " 1  2025-03-15 16:30:40        5      funny  \n",
       " 2  2025-03-18 04:43:41        1  AskReddit  \n",
       " 3  2025-03-18 12:48:15       87     gaming  \n",
       " 4  2025-03-18 12:43:17        1     soccer  )"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "file_path = \"reduced_output.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Display basic info and the first few rows\n",
    "df_info = df.info()\n",
    "df_head = df.head()\n",
    "\n",
    "df_info, df_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8d9d0e87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Upvotes",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "550420c9-84c0-4dec-9ca1-14904e22df3b",
       "rows": [
        [
         "count",
         "7100.0"
        ],
        [
         "mean",
         "84.8194366197183"
        ],
        [
         "std",
         "1866.138000405368"
        ],
        [
         "min",
         "-167.0"
        ],
        [
         "25%",
         "1.0"
        ],
        [
         "50%",
         "2.0"
        ],
        [
         "75%",
         "10.0"
        ],
        [
         "max",
         "104279.0"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 8
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Upvotes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>7100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>84.819437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1866.138000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-167.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>104279.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Upvotes\n",
       "count    7100.000000\n",
       "mean       84.819437\n",
       "std      1866.138000\n",
       "min      -167.000000\n",
       "25%         1.000000\n",
       "50%         2.000000\n",
       "75%        10.000000\n",
       "max    104279.000000"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_eda = df.copy()\n",
    "df_eda.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6bd97850",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eda[['Text', 'Category', 'Subreddit']].to_csv('target_data_2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "24b9c245",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text         43\n",
       "Category      0\n",
       "Date          0\n",
       "Upvotes       0\n",
       "Subreddit     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_eda.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3dd950e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(3)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_eda.duplicated().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "115934a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eda = df_eda.dropna(subset=['Text'])\n",
    "\n",
    "df_eda = df_eda.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0fda0be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)  # hapus URL\n",
    "    text = re.sub(r\"[^a-z\\s]\", \"\", text)  # hapus semua non-huruf\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()  # hapus spasi berlebih\n",
    "    return text\n",
    "\n",
    "df_eda[\"Text\"] = df_eda[\"Text\"].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "29f02ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eda['TextLength'] = df_eda['Text'].apply(len)\n",
    "\n",
    "lower_bound = df_eda['TextLength'].quantile(0.01)\n",
    "upper_bound = df_eda['TextLength'].quantile(0.99)\n",
    "\n",
    "df_eda = df_eda[(df_eda['TextLength'] >= lower_bound) & (df_eda['TextLength'] <= upper_bound)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "91c56190",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "TextLength",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "f3edcd8f-4821-48a5-b06b-370dd0ff68c3",
       "rows": [
        [
         "count",
         "6920.0"
        ],
        [
         "mean",
         "125.9728323699422"
        ],
        [
         "std",
         "146.2095821994517"
        ],
        [
         "min",
         "3.0"
        ],
        [
         "25%",
         "32.0"
        ],
        [
         "50%",
         "73.0"
        ],
        [
         "75%",
         "160.0"
        ],
        [
         "max",
         "956.0"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 8
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TextLength</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>6920.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>125.972832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>146.209582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>32.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>73.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>160.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>956.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        TextLength\n",
       "count  6920.000000\n",
       "mean    125.972832\n",
       "std     146.209582\n",
       "min       3.000000\n",
       "25%      32.000000\n",
       "50%      73.000000\n",
       "75%     160.000000\n",
       "max     956.000000"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_eda.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "657dda2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eda.to_csv(\"target_data_cleaned\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "049fffd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eda[['Text', 'Subreddit']].to_csv('target_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73033abf",
   "metadata": {},
   "source": [
    "# Data Preprocessing & Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "d5b08d9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1010 entries, 0 to 1009\n",
      "Data columns (total 5 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   Text       1002 non-null   object\n",
      " 1   Category   1010 non-null   object\n",
      " 2   Date       1010 non-null   object\n",
      " 3   Upvotes    1010 non-null   int64 \n",
      " 4   Subreddit  1010 non-null   object\n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 39.6+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None,\n",
       "                                                 Text  Category  \\\n",
       " 0  This man is a cut above the rest, outdoor or i...  No Slang   \n",
       " 1                                      Why not four?  No Slang   \n",
       " 2                                          MAGA baby  No Slang   \n",
       " 3  I doubt even ChatGPT would come up with a stor...  No Slang   \n",
       " 4  I feel that's kinda true but at the same time ...  No Slang   \n",
       " \n",
       "                   Date  Upvotes  Subreddit  \n",
       " 0  2025-03-18 12:36:49        1     soccer  \n",
       " 1  2025-03-15 16:30:40        5      funny  \n",
       " 2  2025-03-18 04:43:41        1  AskReddit  \n",
       " 3  2025-03-18 12:48:15       87     gaming  \n",
       " 4  2025-03-18 12:43:17        1     soccer  )"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "file_path = \"reduced_output.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Display basic info and the first few rows\n",
    "df_info = df.info()\n",
    "df_head = df.head()\n",
    "\n",
    "df_info, df_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "80c5f1e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\andif\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\andif\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Processed_Text",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Category",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Category_Label",
         "rawType": "int8",
         "type": "integer"
        },
        {
         "name": "Date",
         "rawType": "datetime64[ns]",
         "type": "datetime"
        },
        {
         "name": "Upvotes",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Subreddit",
         "rawType": "object",
         "type": "string"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "abb6d6a9-d818-4387-99cd-bee4ba24b17a",
       "rows": [
        [
         "0",
         "man cut rest outdoor inside",
         "No Slang",
         "0",
         "2025-03-18 12:36:49",
         "1",
         "soccer"
        ],
        [
         "1",
         "four",
         "No Slang",
         "0",
         "2025-03-15 16:30:40",
         "5",
         "funny"
        ],
        [
         "2",
         "maga baby",
         "No Slang",
         "0",
         "2025-03-18 04:43:41",
         "1",
         "AskReddit"
        ],
        [
         "3",
         "doubt even chatgpt would come story stupid bunch hipsters become criminals killers order pay student loans",
         "No Slang",
         "0",
         "2025-03-18 12:48:15",
         "87",
         "gaming"
        ],
        [
         "4",
         "feel thats kinda true time also true smaller teams also benefit extremly rulechange play intense football subs sure city bring worldclass players smaller team fit players already help lot",
         "No Slang",
         "0",
         "2025-03-18 12:43:17",
         "1",
         "soccer"
        ]
       ],
       "shape": {
        "columns": 6,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Processed_Text</th>\n",
       "      <th>Category</th>\n",
       "      <th>Category_Label</th>\n",
       "      <th>Date</th>\n",
       "      <th>Upvotes</th>\n",
       "      <th>Subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>man cut rest outdoor inside</td>\n",
       "      <td>No Slang</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-03-18 12:36:49</td>\n",
       "      <td>1</td>\n",
       "      <td>soccer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>four</td>\n",
       "      <td>No Slang</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-03-15 16:30:40</td>\n",
       "      <td>5</td>\n",
       "      <td>funny</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>maga baby</td>\n",
       "      <td>No Slang</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-03-18 04:43:41</td>\n",
       "      <td>1</td>\n",
       "      <td>AskReddit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>doubt even chatgpt would come story stupid bun...</td>\n",
       "      <td>No Slang</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-03-18 12:48:15</td>\n",
       "      <td>87</td>\n",
       "      <td>gaming</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>feel thats kinda true time also true smaller t...</td>\n",
       "      <td>No Slang</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-03-18 12:43:17</td>\n",
       "      <td>1</td>\n",
       "      <td>soccer</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Processed_Text  Category  \\\n",
       "0                        man cut rest outdoor inside  No Slang   \n",
       "1                                               four  No Slang   \n",
       "2                                          maga baby  No Slang   \n",
       "3  doubt even chatgpt would come story stupid bun...  No Slang   \n",
       "4  feel thats kinda true time also true smaller t...  No Slang   \n",
       "\n",
       "   Category_Label                Date  Upvotes  Subreddit  \n",
       "0               0 2025-03-18 12:36:49        1     soccer  \n",
       "1               0 2025-03-15 16:30:40        5      funny  \n",
       "2               0 2025-03-18 04:43:41        1  AskReddit  \n",
       "3               0 2025-03-18 12:48:15       87     gaming  \n",
       "4               0 2025-03-18 12:43:17        1     soccer  "
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download NLTK resources (if not already installed)\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Salin dataframe untuk diolah\n",
    "df_cleaned = df.copy()\n",
    "\n",
    "# --- Step 1: Data Cleaning ---\n",
    "# Hapus baris dengan Text yang kosong\n",
    "df_cleaned = df_cleaned.dropna(subset=['Text'])\n",
    "\n",
    "# Hapus duplikat (jika ada)\n",
    "df_cleaned = df_cleaned.drop_duplicates()\n",
    "\n",
    "# Hapus URL, simbol aneh, dan lowercase\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)  # hapus URL\n",
    "    text = re.sub(r\"[^a-z\\s]\", \"\", text)  # hapus semua non-huruf\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()  # hapus spasi berlebih\n",
    "    return text\n",
    "\n",
    "df_cleaned[\"Cleaned_Text\"] = df_cleaned[\"Text\"].apply(clean_text)\n",
    "\n",
    "# --- Step 2: Tokenization & Stopwords Removal ---\n",
    "# Tokenisasi dan menghapus stopwords\n",
    "stop_words = set(stopwords.words(\"english\"))  # set of common stopwords\n",
    "\n",
    "def tokenize_and_remove_stopwords(text):\n",
    "    # Tokenisasi\n",
    "    tokens = word_tokenize(text)\n",
    "    # Hapus stopwords\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "    return \" \".join(filtered_tokens)\n",
    "\n",
    "df_cleaned[\"Processed_Text\"] = df_cleaned[\"Cleaned_Text\"].apply(tokenize_and_remove_stopwords)\n",
    "\n",
    "# --- Step 3: Data Transformation ---\n",
    "# Ubah kolom Date ke datetime\n",
    "df_cleaned[\"Date\"] = pd.to_datetime(df_cleaned[\"Date\"], errors=\"coerce\")\n",
    "\n",
    "# --- Step 4: Encoding untuk klasifikasi nanti ---\n",
    "# Label encoding kategori dan subreddit\n",
    "df_cleaned[\"Category_Label\"] = df_cleaned[\"Category\"].astype(\"category\").cat.codes\n",
    "\n",
    "# Tampilkan hasil akhir sebagian\n",
    "df_cleaned[[\"Processed_Text\", \"Category\", \"Category_Label\", \"Date\",\"Upvotes\", \"Subreddit\"]].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "3872c917",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1002, 1000)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# --- Step 4: Text Vectorization (TF-IDF) ---\n",
    "# Gunakan TF-IDF pada Cleaned_Text\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_features=1000,  # ambil 1000 kata paling penting\n",
    "    stop_words='english',  # hilangkan stop words\n",
    "    ngram_range=(1, 2)  # unigram dan bigram\n",
    ")\n",
    "\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df_cleaned[\"Cleaned_Text\"])\n",
    "\n",
    "# Bentuk dari hasil TF-IDF\n",
    "tfidf_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "f543bbcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   Text  Category  \\\n",
      "0     This man is a cut above the rest, outdoor or i...  No Slang   \n",
      "1                                         Why not four?  No Slang   \n",
      "2                                             MAGA baby  No Slang   \n",
      "3     I doubt even ChatGPT would come up with a stor...  No Slang   \n",
      "4     I feel that's kinda true but at the same time ...  No Slang   \n",
      "...                                                 ...       ...   \n",
      "1005  A sounds better. Also sounds like a way to get...  No Slang   \n",
      "1006  embarrassed watching this. \\n\\nGreat finish fr...  No Slang   \n",
      "1007  Nervous and awkward. Don't expect it to be mag...  No Slang   \n",
      "1008                                           Feminine  No Slang   \n",
      "1009                                       Omg, genius.  No Slang   \n",
      "\n",
      "                    Date  Upvotes          Subreddit  \\\n",
      "0    2025-03-18 12:36:49        1             soccer   \n",
      "1    2025-03-15 16:30:40        5              funny   \n",
      "2    2025-03-18 04:43:41        1          AskReddit   \n",
      "3    2025-03-18 12:48:15       87             gaming   \n",
      "4    2025-03-18 12:43:17        1             soccer   \n",
      "...                  ...      ...                ...   \n",
      "1005 2025-03-18 19:29:52        2  NoStupidQuestions   \n",
      "1006 2025-03-16 17:49:25        2             soccer   \n",
      "1007 2025-03-17 14:55:25     1470  NoStupidQuestions   \n",
      "1008 2025-03-18 17:06:16        6  NoStupidQuestions   \n",
      "1009 2025-03-15 00:17:05        4             gaming   \n",
      "\n",
      "                                           Cleaned_Text  \\\n",
      "0     this man is a cut above the rest outdoor or in...   \n",
      "1                                          why not four   \n",
      "2                                             maga baby   \n",
      "3     i doubt even chatgpt would come up with a stor...   \n",
      "4     i feel thats kinda true but at the same time a...   \n",
      "...                                                 ...   \n",
      "1005  a sounds better also sounds like a way to get ...   \n",
      "1006   embarrassed watching this great finish from isak   \n",
      "1007  nervous and awkward dont expect it to be magic...   \n",
      "1008                                           feminine   \n",
      "1009                                         omg genius   \n",
      "\n",
      "                                         Processed_Text  Category_Label  \n",
      "0                           man cut rest outdoor inside               0  \n",
      "1                                                  four               0  \n",
      "2                                             maga baby               0  \n",
      "3     doubt even chatgpt would come story stupid bun...               0  \n",
      "4     feel thats kinda true time also true smaller t...               0  \n",
      "...                                                 ...             ...  \n",
      "1005  sounds better also sounds like way get date ev...               0  \n",
      "1006             embarrassed watching great finish isak               0  \n",
      "1007  nervous awkward dont expect magical impressive...               0  \n",
      "1008                                           feminine               0  \n",
      "1009                                         omg genius               0  \n",
      "\n",
      "[1002 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9cc8d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# 1. Split data\n",
    "X = df_cleaned[\"Cleaned_Text\"]\n",
    "y = df_cleaned[\"Category_Label\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 2. Build pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(max_features=1000, stop_words='english', ngram_range=(1,2))),\n",
    "    ('clf', LogisticRegression(max_iter=1000))\n",
    "])\n",
    "\n",
    "# 3. Train model\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# 4. Predict & evaluate\n",
    "y_pred = pipeline.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39afd27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Data\n",
    "X = df_cleaned[\"Cleaned_Text\"]\n",
    "y = df_cleaned[\"Category_Label\"]\n",
    "\n",
    "# Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Model pipeline templates\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
    "    \"Naive Bayes\": MultinomialNB(),\n",
    "    \"Linear SVM\": LinearSVC()\n",
    "}\n",
    "\n",
    "# Loop through models\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n=== {name} ===\")\n",
    "    \n",
    "    pipeline = Pipeline([\n",
    "        (\"tfidf\", TfidfVectorizer(max_features=1000, stop_words='english', ngram_range=(1,2))),\n",
    "        (\"clf\", model)\n",
    "    ])\n",
    "    \n",
    "    pipeline.fit(X_train, y_train)\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    \n",
    "    print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "    print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "163152f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data reduced and saved to Labelled_1500.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def reduce_csv_to_1000_rows(input_csv, output_csv):\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(input_csv)\n",
    "\n",
    "    # If the dataset has more than 1000 rows, randomly sample 1000 rows\n",
    "    if len(df) > 1000:\n",
    "        df_reduced = df.sample(n=1500, random_state=42)  # random_state for reproducibility\n",
    "    else:\n",
    "        df_reduced = df  # if there are fewer than 1000 rows, keep all rows\n",
    "\n",
    "    # Save the reduced dataset to a new CSV file\n",
    "    df_reduced[[\"Text\", \"Subreddit\", \"Category\"]].to_csv(output_csv, index=False)\n",
    "\n",
    "    print(f\"Data reduced and saved to {output_csv}\")\n",
    "\n",
    "# Example usage\n",
    "input_csv = 'C:/personal/Code/S6/NLP/Slang Classification/Datas/Labelled.csv'  # Replace with your input CSV file path\n",
    "output_csv = 'Labelled_1500.csv'  # Replace with your desired output file path\n",
    "reduce_csv_to_1000_rows(input_csv, output_csv)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
