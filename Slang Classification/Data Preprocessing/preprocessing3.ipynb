{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe81d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def reduce_csv_to_1000_rows(input_csv, output_csv):\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(input_csv)\n",
    "\n",
    "    # If the dataset has more than 1000 rows, randomly sample 1000 rows\n",
    "    if len(df) > 1000:\n",
    "        df_reduced = df.sample(n=1000, random_state=42)  # random_state for reproducibility\n",
    "    else:\n",
    "        df_reduced = df  # if there are fewer than 1000 rows, keep all rows\n",
    "\n",
    "    # Save the reduced dataset to a new CSV file\n",
    "    df_reduced.to_csv(output_csv, index=False)\n",
    "\n",
    "    print(f\"Data reduced and saved to {output_csv}\")\n",
    "\n",
    "# Example usage\n",
    "input_csv = 'cleaned_data.csv'  # Replace with your input CSV file path\n",
    "output_csv = '1000_rows_data.csv'  # Replace with your desired output file path\n",
    "reduce_csv_to_1000_rows(input_csv, output_csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47744748",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "df = pd.read_csv('reduced_output_file.csv')  # Replace 'your_file.csv' with your actual file path\n",
    "\n",
    "# Create DataFrame\n",
    "post_data = pd.DataFrame(df)\n",
    "\n",
    "# Step 1: Combine Title, Text, and Comments Data into separate rows\n",
    "\n",
    "# Convert Comments Data from string format to list\n",
    "post_data['Comments Data'] = post_data['Comments Data'].apply(ast.literal_eval)\n",
    "\n",
    "# Prepare the DataFrame for expanding to multiple rows\n",
    "expanded_data = []\n",
    "\n",
    "# Add the Title and Text as separate rows\n",
    "for _, row in post_data.iterrows():\n",
    "    # Add the title as a row\n",
    "    expanded_data.append({'Text': row['Title'], 'Category': 'No Slang', 'Date': datetime.fromtimestamp(row['Timestamp'], tz=timezone.utc).strftime('%Y-%m-%d %H:%M:%S'), 'Subreddit': row['Subreddit']})\n",
    "    \n",
    "    # Add the text (if available)\n",
    "    if row['Text']:\n",
    "        expanded_data.append({'Text': row['Text'], 'Category': 'No Slang', 'Date': datetime.fromtimestamp(row['Timestamp'], tz=timezone.utc).strftime('%Y-%m-%d %H:%M:%S'), 'Subreddit': row['Subreddit']})\n",
    "    \n",
    "    # Add each comment as a separate row\n",
    "    for comment in row['Comments Data']:\n",
    "        expanded_data.append({'Text': comment[0], 'Category': 'No Slang', 'Date': datetime.fromtimestamp(comment[2], tz=timezone.utc).strftime('%Y-%m-%d %H:%M:%S'), 'Subreddit': row['Subreddit']})\n",
    "\n",
    "# Create the expanded DataFrame\n",
    "expanded_df = pd.DataFrame(expanded_data)\n",
    "\n",
    "expanded_df.to_csv('cleaned_data.csv', index=False)  # Saves the cleaned data to a new file\n",
    "\n",
    "print(expanded_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b79df422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 4 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   Text       992 non-null    object\n",
      " 1   Category   1000 non-null   object\n",
      " 2   Date       1000 non-null   object\n",
      " 3   Subreddit  1000 non-null   object\n",
      "dtypes: object(4)\n",
      "memory usage: 31.4+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None,\n",
       "                                                 Text  Category  \\\n",
       " 0  This man is a cut above the rest, outdoor or i...  No Slang   \n",
       " 1                                      Why not four?  No Slang   \n",
       " 2                                          MAGA baby  No Slang   \n",
       " 3  I doubt even ChatGPT would come up with a stor...  No Slang   \n",
       " 4  I feel that's kinda true but at the same time ...  No Slang   \n",
       " \n",
       "                   Date  Subreddit  \n",
       " 0  2025-03-18 12:36:49     soccer  \n",
       " 1  2025-03-15 16:30:40      funny  \n",
       " 2  2025-03-18 04:43:41  AskReddit  \n",
       " 3  2025-03-18 12:48:15     gaming  \n",
       " 4  2025-03-18 12:43:17     soccer  )"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "file_path = \"1000_rows_data.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Display basic info and the first few rows\n",
    "df_info = df.info()\n",
    "df_head = df.head()\n",
    "\n",
    "df_info, df_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "80c5f1e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\andif\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\andif\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Processed_Text",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Category",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Category_Label",
         "rawType": "int8",
         "type": "integer"
        },
        {
         "name": "Date",
         "rawType": "datetime64[ns]",
         "type": "datetime"
        },
        {
         "name": "Subreddit",
         "rawType": "object",
         "type": "string"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "7bad3cea-5107-4cb2-839f-280c0f565b8a",
       "rows": [
        [
         "0",
         "man cut rest outdoor inside gon na wan na lol lit",
         "No Slang",
         "0",
         "2025-03-18 12:36:49",
         "soccer"
        ],
        [
         "1",
         "four",
         "No Slang",
         "0",
         "2025-03-15 16:30:40",
         "funny"
        ],
        [
         "2",
         "maga baby",
         "No Slang",
         "0",
         "2025-03-18 04:43:41",
         "AskReddit"
        ],
        [
         "3",
         "doubt even chatgpt would come story stupid bunch hipsters become criminals killers order pay student loans",
         "No Slang",
         "0",
         "2025-03-18 12:48:15",
         "gaming"
        ],
        [
         "4",
         "feel thats kinda true time also true smaller teams also benefit extremly rulechange play intense football subs sure city bring worldclass players smaller team fit players already help lot",
         "No Slang",
         "0",
         "2025-03-18 12:43:17",
         "soccer"
        ]
       ],
       "shape": {
        "columns": 5,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Processed_Text</th>\n",
       "      <th>Category</th>\n",
       "      <th>Category_Label</th>\n",
       "      <th>Date</th>\n",
       "      <th>Subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>man cut rest outdoor inside gon na wan na lol lit</td>\n",
       "      <td>No Slang</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-03-18 12:36:49</td>\n",
       "      <td>soccer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>four</td>\n",
       "      <td>No Slang</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-03-15 16:30:40</td>\n",
       "      <td>funny</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>maga baby</td>\n",
       "      <td>No Slang</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-03-18 04:43:41</td>\n",
       "      <td>AskReddit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>doubt even chatgpt would come story stupid bun...</td>\n",
       "      <td>No Slang</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-03-18 12:48:15</td>\n",
       "      <td>gaming</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>feel thats kinda true time also true smaller t...</td>\n",
       "      <td>No Slang</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-03-18 12:43:17</td>\n",
       "      <td>soccer</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Processed_Text  Category  \\\n",
       "0  man cut rest outdoor inside gon na wan na lol lit  No Slang   \n",
       "1                                               four  No Slang   \n",
       "2                                          maga baby  No Slang   \n",
       "3  doubt even chatgpt would come story stupid bun...  No Slang   \n",
       "4  feel thats kinda true time also true smaller t...  No Slang   \n",
       "\n",
       "   Category_Label                Date  Subreddit  \n",
       "0               0 2025-03-18 12:36:49     soccer  \n",
       "1               0 2025-03-15 16:30:40      funny  \n",
       "2               0 2025-03-18 04:43:41  AskReddit  \n",
       "3               0 2025-03-18 12:48:15     gaming  \n",
       "4               0 2025-03-18 12:43:17     soccer  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download NLTK resources (if not already installed)\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Salin dataframe untuk diolah\n",
    "df_cleaned = df.copy()\n",
    "\n",
    "# --- Step 1: Data Cleaning ---\n",
    "# Hapus baris dengan Text yang kosong\n",
    "df_cleaned = df_cleaned.dropna(subset=['Text'])\n",
    "\n",
    "# Hapus duplikat (jika ada)\n",
    "df_cleaned = df_cleaned.drop_duplicates()\n",
    "\n",
    "# Hapus URL, simbol aneh, dan lowercase\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)  # hapus URL\n",
    "    text = re.sub(r\"[^a-z\\s]\", \"\", text)  # hapus semua non-huruf\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()  # hapus spasi berlebih\n",
    "    return text\n",
    "\n",
    "df_cleaned[\"Cleaned_Text\"] = df_cleaned[\"Text\"].apply(clean_text)\n",
    "\n",
    "# --- Step 2: Tokenization & Stopwords Removal ---\n",
    "# Tokenisasi dan menghapus stopwords\n",
    "stop_words = set(stopwords.words(\"english\"))  # set of common stopwords\n",
    "\n",
    "def tokenize_and_remove_stopwords(text):\n",
    "    # Tokenisasi\n",
    "    tokens = word_tokenize(text)\n",
    "    # Hapus stopwords\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "    return \" \".join(filtered_tokens)\n",
    "\n",
    "df_cleaned[\"Processed_Text\"] = df_cleaned[\"Cleaned_Text\"].apply(tokenize_and_remove_stopwords)\n",
    "\n",
    "# --- Step 3: Data Transformation ---\n",
    "# Ubah kolom Date ke datetime\n",
    "df_cleaned[\"Date\"] = pd.to_datetime(df_cleaned[\"Date\"], errors=\"coerce\")\n",
    "\n",
    "# --- Step 4: Encoding untuk klasifikasi nanti ---\n",
    "# Label encoding kategori dan subreddit\n",
    "df_cleaned[\"Category_Label\"] = df_cleaned[\"Category\"].astype(\"category\").cat.codes\n",
    "\n",
    "# Tampilkan hasil akhir sebagian\n",
    "df_cleaned[[\"Processed_Text\", \"Category\", \"Category_Label\", \"Date\", \"Subreddit\"]].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3872c917",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(992, 1000)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# --- Step 4: Text Vectorization (TF-IDF) ---\n",
    "# Gunakan TF-IDF pada Cleaned_Text\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_features=1000,  # ambil 1000 kata paling penting\n",
    "    stop_words='english',  # hilangkan stop words\n",
    "    ngram_range=(1, 2)  # unigram dan bigram\n",
    ")\n",
    "\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df_cleaned[\"Cleaned_Text\"])\n",
    "\n",
    "# Bentuk dari hasil TF-IDF\n",
    "tfidf_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e9cc8d30",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "This solver needs samples of at least 2 classes in the data, but the data contains only one class: np.int8(0)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m     14\u001b[39m pipeline = Pipeline([\n\u001b[32m     15\u001b[39m     (\u001b[33m'\u001b[39m\u001b[33mtfidf\u001b[39m\u001b[33m'\u001b[39m, TfidfVectorizer(max_features=\u001b[32m1000\u001b[39m, stop_words=\u001b[33m'\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m'\u001b[39m, ngram_range=(\u001b[32m1\u001b[39m,\u001b[32m2\u001b[39m))),\n\u001b[32m     16\u001b[39m     (\u001b[33m'\u001b[39m\u001b[33mclf\u001b[39m\u001b[33m'\u001b[39m, LogisticRegression(max_iter=\u001b[32m1000\u001b[39m))\n\u001b[32m     17\u001b[39m ])\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# 3. Train model\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# 4. Predict & evaluate\u001b[39;00m\n\u001b[32m     23\u001b[39m y_pred = pipeline.predict(X_test)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\personal\\Code\\S6\\NLP\\.venv\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1382\u001b[39m     estimator._validate_params()\n\u001b[32m   1384\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1385\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1386\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1387\u001b[39m     )\n\u001b[32m   1388\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1389\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\personal\\Code\\S6\\NLP\\.venv\\Lib\\site-packages\\sklearn\\pipeline.py:662\u001b[39m, in \u001b[36mPipeline.fit\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m    656\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._final_estimator != \u001b[33m\"\u001b[39m\u001b[33mpassthrough\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    657\u001b[39m         last_step_params = \u001b[38;5;28mself\u001b[39m._get_metadata_for_step(\n\u001b[32m    658\u001b[39m             step_idx=\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m) - \u001b[32m1\u001b[39m,\n\u001b[32m    659\u001b[39m             step_params=routed_params[\u001b[38;5;28mself\u001b[39m.steps[-\u001b[32m1\u001b[39m][\u001b[32m0\u001b[39m]],\n\u001b[32m    660\u001b[39m             all_params=params,\n\u001b[32m    661\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m662\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_final_estimator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mlast_step_params\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfit\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    664\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\personal\\Code\\S6\\NLP\\.venv\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1382\u001b[39m     estimator._validate_params()\n\u001b[32m   1384\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1385\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1386\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1387\u001b[39m     )\n\u001b[32m   1388\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1389\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\personal\\Code\\S6\\NLP\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1301\u001b[39m, in \u001b[36mLogisticRegression.fit\u001b[39m\u001b[34m(self, X, y, sample_weight)\u001b[39m\n\u001b[32m   1299\u001b[39m classes_ = \u001b[38;5;28mself\u001b[39m.classes_\n\u001b[32m   1300\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m n_classes < \u001b[32m2\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m1301\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1302\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThis solver needs samples of at least 2 classes\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1303\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m in the data, but the data contains only one\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1304\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m class: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[33m\"\u001b[39m % classes_[\u001b[32m0\u001b[39m]\n\u001b[32m   1305\u001b[39m     )\n\u001b[32m   1307\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.classes_) == \u001b[32m2\u001b[39m:\n\u001b[32m   1308\u001b[39m     n_classes = \u001b[32m1\u001b[39m\n",
      "\u001b[31mValueError\u001b[39m: This solver needs samples of at least 2 classes in the data, but the data contains only one class: np.int8(0)"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# 1. Split data\n",
    "X = df_cleaned[\"Cleaned_Text\"]\n",
    "y = df_cleaned[\"Category_Label\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 2. Build pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(max_features=1000, stop_words='english', ngram_range=(1,2))),\n",
    "    ('clf', LogisticRegression(max_iter=1000))\n",
    "])\n",
    "\n",
    "# 3. Train model\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# 4. Predict & evaluate\n",
    "y_pred = pipeline.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39afd27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Data\n",
    "X = df_cleaned[\"Cleaned_Text\"]\n",
    "y = df_cleaned[\"Category_Label\"]\n",
    "\n",
    "# Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Model pipeline templates\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
    "    \"Naive Bayes\": MultinomialNB(),\n",
    "    \"Linear SVM\": LinearSVC()\n",
    "}\n",
    "\n",
    "# Loop through models\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n=== {name} ===\")\n",
    "    \n",
    "    pipeline = Pipeline([\n",
    "        (\"tfidf\", TfidfVectorizer(max_features=1000, stop_words='english', ngram_range=(1,2))),\n",
    "        (\"clf\", model)\n",
    "    ])\n",
    "    \n",
    "    pipeline.fit(X_train, y_train)\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    \n",
    "    print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "    print(classification_report(y_test, y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
